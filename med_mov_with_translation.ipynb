{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7855edb7-f6d7-4245-8555-3989d509f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 786.4/992.0 kB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 992.0/992.0 kB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9df8f33-50c7-4bea-b232-d9b82268743b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.4.26)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.4 MB/s eta 0:00:00\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17516 sha256=505232c3fc959a91e3515cfa9b9e07c827c377c3b2fa2e8ea7eb8baca0adb00b\n",
      "  Stored in directory: c:\\users\\ma698\\appdata\\local\\pip\\cache\\wheels\\95\\0f\\04\\b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab 4.2.5 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0194ee43-5211-488e-8170-5e8c5773ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: jiwer in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from jiwer) (3.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ma698\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ma698\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchaudio jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b0d944-99f5-453a-9954-1b3e4dd99eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASR model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9bebd2fea54db5a2bd9a356a7a874d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 403 files:   0%|          | 0/403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR model loaded successfully!\n",
      "Loading translation model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 272\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menglish_translation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# Run main program\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[6], line 143\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m asr_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indic-conformer-600m-multilingual\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASR model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m translator \u001b[38;5;241m=\u001b[39m load_translation_model()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Load ground truth data (if available)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m grounds_truths \u001b[38;5;241m=\u001b[39m load_grounds_truth(gt_file)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mload_translation_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Option 1: Using Helsinki-NLP model (recommended)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-hi-en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     18\u001b[0m translation_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create translation pipeline\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1037\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1037\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1038\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1039\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m             )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from jiwer import wer, cer\n",
    "import warnings\n",
    "\n",
    "# Ignore CUDA provider warnings since we're running on CPU\n",
    "warnings.filterwarnings(\"ignore\", message=\".*CUDAExecutionProvider.*\")\n",
    "\n",
    "def load_translation_model():\n",
    "    \"\"\"Load Hindi to English translation model\"\"\"\n",
    "    print(\"Loading translation model...\")\n",
    "    \n",
    "    # Option 1: Using Helsinki-NLP model (recommended)\n",
    "    model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    translation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create translation pipeline\n",
    "    translator = pipeline(\"translation\", \n",
    "                         model=translation_model, \n",
    "                         tokenizer=tokenizer,\n",
    "                         device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    print(\"Translation model loaded successfully!\")\n",
    "    return translator\n",
    "\n",
    "def translate_text(translator, hindi_text):\n",
    "    \"\"\"Translate Hindi text to English\"\"\"\n",
    "    try:\n",
    "        # Split long text into chunks if needed (model has token limits)\n",
    "        max_length = 400  # Adjust based on model capacity\n",
    "        \n",
    "        if len(hindi_text) <= max_length:\n",
    "            result = translator(hindi_text, max_length=512, num_beams=4, early_stopping=True)\n",
    "            return result[0]['translation_text']\n",
    "        else:\n",
    "            # Split into sentences and translate each\n",
    "            sentences = hindi_text.split('।')  # Hindi sentence delimiter\n",
    "            if len(sentences) == 1:\n",
    "                sentences = hindi_text.split('.')  # Fallback to period\n",
    "            \n",
    "            translated_sentences = []\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if sentence:\n",
    "                    result = translator(sentence, max_length=512, num_beams=4, early_stopping=True)\n",
    "                    translated_sentences.append(result[0]['translation_text'])\n",
    "            \n",
    "            return ' '.join(translated_sentences)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_grounds_truth(file_path):\n",
    "    \"\"\"Load ground truth transcriptions from file\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Ground truth file '{file_path}' not found!\")\n",
    "        return {}\n",
    "    \n",
    "    grounds_truths = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Handle different file formats\n",
    "            if \"|\" in line:\n",
    "                parts = line.split(\"|\", 1)\n",
    "                audio_file, transcription = parts[0].strip(), parts[1].strip()\n",
    "                if not audio_file:\n",
    "                    audio_file = \"audio.flac\"\n",
    "            else:\n",
    "                # No separator found, assume transcription for default audio file\n",
    "                audio_file = \"audio.flac\"\n",
    "                transcription = line\n",
    "            \n",
    "            grounds_truths[audio_file] = transcription\n",
    "    \n",
    "    return grounds_truths\n",
    "\n",
    "def preprocess_audio(audio_path, target_sr=16000):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Convert stereo to mono if needed\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        return waveform, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(model, waveform):\n",
    "    \"\"\"Transcribe audio using the ASR model\"\"\"\n",
    "    # Try different model calling approaches\n",
    "    approaches = [\n",
    "        lambda: model(waveform, \"hi\", \"ctc\"),\n",
    "        lambda: model(waveform.squeeze(), \"hi\", \"ctc\"),\n",
    "        lambda: model(waveform.unsqueeze(0), \"hi\", \"ctc\"),\n",
    "    ]\n",
    "    \n",
    "    for approach in approaches:\n",
    "        try:\n",
    "            result = approach()\n",
    "            return result if isinstance(result, str) else str(result)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"All transcription approaches failed\")\n",
    "\n",
    "def save_translations(translations, output_file=\"translations.txt\"):\n",
    "    \"\"\"Save translations to file\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for audio_file, data in translations.items():\n",
    "            f.write(f\"Audio File: {audio_file}\\n\")\n",
    "            f.write(f\"Hindi: {data['hindi']}\\n\")\n",
    "            f.write(f\"English: {data['english']}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "    print(f\"Translations saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    gt_file = \"grounds_truth.txt\"\n",
    "    target_sr = 16000\n",
    "    output_file = \"translations.txt\"\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading ASR model...\")\n",
    "    asr_model = AutoModel.from_pretrained(\"ai4bharat/indic-conformer-600m-multilingual\", trust_remote_code=True)\n",
    "    print(\"ASR model loaded successfully!\")\n",
    "    \n",
    "    translator = load_translation_model()\n",
    "    \n",
    "    # Load ground truth data (if available)\n",
    "    grounds_truths = load_grounds_truth(gt_file)\n",
    "    \n",
    "    # Dictionary to store all translations\n",
    "    all_translations = {}\n",
    "    \n",
    "    # If we have ground truth, process those files\n",
    "    if grounds_truths:\n",
    "        print(f\"\\nFound {len(grounds_truths)} audio file(s) to process\\n\")\n",
    "        \n",
    "        for audio_file, gt_text in grounds_truths.items():\n",
    "            print(f\"Processing: {audio_file}\")\n",
    "            \n",
    "            # Find audio file\n",
    "            if not os.path.exists(audio_file):\n",
    "                print(f\"Audio file not found: {audio_file}\")\n",
    "                # If we have ground truth text, translate it anyway\n",
    "                print(\"Using ground truth text for translation...\")\n",
    "                hindi_text = gt_text\n",
    "            else:\n",
    "                # Load and transcribe audio\n",
    "                waveform, original_sr = preprocess_audio(audio_file, target_sr)\n",
    "                if waveform is None:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Audio loaded: {waveform.shape[1] / target_sr:.1f}s at {original_sr}Hz\")\n",
    "                \n",
    "                try:\n",
    "                    hindi_text = transcribe_audio(asr_model, waveform)\n",
    "                    hindi_text = hindi_text.strip()\n",
    "                    print(f\"Transcription: {hindi_text}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Transcription failed: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Translate to English\n",
    "            print(\"Translating to English...\")\n",
    "            english_text = translate_text(translator, hindi_text)\n",
    "            \n",
    "            if english_text:\n",
    "                print(f\"Translation: {english_text}\")\n",
    "                all_translations[audio_file] = {\n",
    "                    'hindi': hindi_text,\n",
    "                    'english': english_text\n",
    "                }\n",
    "                \n",
    "                # Calculate WER/CER if ground truth available\n",
    "                if gt_text:\n",
    "                    sample_wer = wer(gt_text, hindi_text)\n",
    "                    sample_cer = cer(gt_text, hindi_text)\n",
    "                    print(f\"WER: {sample_wer:.3f} | CER: {sample_cer:.3f}\")\n",
    "            else:\n",
    "                print(\"Translation failed!\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    else:\n",
    "        # Process default audio file if no ground truth\n",
    "        audio_file = \"audio.flac\"\n",
    "        if os.path.exists(audio_file):\n",
    "            print(f\"Processing: {audio_file}\")\n",
    "            \n",
    "            waveform, original_sr = preprocess_audio(audio_file, target_sr)\n",
    "            if waveform is not None:\n",
    "                print(f\"Audio loaded: {waveform.shape[1] / target_sr:.1f}s at {original_sr}Hz\")\n",
    "                \n",
    "                try:\n",
    "                    hindi_text = transcribe_audio(asr_model, waveform)\n",
    "                    hindi_text = hindi_text.strip()\n",
    "                    print(f\"Transcription: {hindi_text}\")\n",
    "                    \n",
    "                    # Translate to English\n",
    "                    print(\"Translating to English...\")\n",
    "                    english_text = translate_text(translator, hindi_text)\n",
    "                    \n",
    "                    if english_text:\n",
    "                        print(f\"Translation: {english_text}\")\n",
    "                        all_translations[audio_file] = {\n",
    "                            'hindi': hindi_text,\n",
    "                            'english': english_text\n",
    "                        }\n",
    "                    else:\n",
    "                        print(\"Translation failed!\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Processing failed: {e}\")\n",
    "        else:\n",
    "            print(\"No audio files found to process!\")\n",
    "    \n",
    "    # Save all translations\n",
    "    if all_translations:\n",
    "        save_translations(all_translations, output_file)\n",
    "        \n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"Files processed: {len(all_translations)}\")\n",
    "        print(f\"Translations saved to: {output_file}\")\n",
    "        \n",
    "        # Display final translations\n",
    "        print(\"\\nFINAL TRANSLATIONS:\")\n",
    "        for audio_file, data in all_translations.items():\n",
    "            print(f\"\\nFile: {audio_file}\")\n",
    "            print(f\"Hindi: {data['hindi']}\")\n",
    "            print(f\"English: {data['english']}\")\n",
    "    else:\n",
    "        print(\"No translations were generated!\")\n",
    "\n",
    "# Alternative function to translate existing Hindi text directly\n",
    "def translate_hindi_text(hindi_text):\n",
    "    \"\"\"Standalone function to translate Hindi text to English\"\"\"\n",
    "    translator = load_translation_model()\n",
    "    english_text = translate_text(translator, hindi_text)\n",
    "    return english_text\n",
    "\n",
    "# Example usage for direct translation\n",
    "def example_direct_translation():\n",
    "    \"\"\"Example of direct translation without audio processing\"\"\"\n",
    "    # Your transcribed Hindi text\n",
    "    hindi_text = \"हेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में खराशे सी लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\"\n",
    "    \n",
    "    print(\"Direct Translation Example:\")\n",
    "    print(f\"Hindi: {hindi_text}\")\n",
    "    \n",
    "    english_translation = translate_hindi_text(hindi_text)\n",
    "    print(f\"English: {english_translation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run main program\n",
    "    main()\n",
    "    \n",
    "    # Uncomment below to run direct translation example\n",
    "    # print(\"\\n\" + \"=\"*80)\n",
    "    # example_direct_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c90348a-5416-4232-99be-f7367dd06b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASR model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6be96d7bba49f0a663e81d178a48f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 403 files:   0%|          | 0/403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR model loaded successfully!\n",
      "Loading translation model...\n",
      "Google Translate loaded successfully!\n",
      "\n",
      "Found 1 audio file(s) to process\n",
      "\n",
      "Processing: audio.flac\n",
      "Audio loaded: 25.7s at 44100Hz\n",
      "Transcription: हेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में हराशें स ही लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\n",
      "Translating to English...\n",
      "Translation: Hello, my name is Mohammad Atar, I have been complaining of cough and cold for three days, my body is also getting pen and there is only a green in the neck, as well as the head is also heavy, so do I have covid or some other disease\n",
      "WER: 0.089 | CER: 0.026\n",
      "--------------------------------------------------------------------------------\n",
      "Translations saved to translations.txt\n",
      "\n",
      "SUMMARY:\n",
      "Files processed: 1\n",
      "Translations saved to: translations.txt\n",
      "\n",
      "FINAL TRANSLATIONS:\n",
      "\n",
      "File: audio.flac\n",
      "Hindi: हेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में हराशें स ही लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\n",
      "English: Hello, my name is Mohammad Atar, I have been complaining of cough and cold for three days, my body is also getting pen and there is only a green in the neck, as well as the head is also heavy, so do I have covid or some other disease\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModel\n",
    "from jiwer import wer, cer\n",
    "import warnings\n",
    "\n",
    "# Ignore CUDA provider warnings since we're running on CPU\n",
    "warnings.filterwarnings(\"ignore\", message=\".*CUDAExecutionProvider.*\")\n",
    "\n",
    "def load_translation_model():\n",
    "    \"\"\"Load translation model with simple fallback options\"\"\"\n",
    "    print(\"Loading translation model...\")\n",
    "    \n",
    "    # Option 1: Try Google Translate (easiest, requires internet)\n",
    "    try:\n",
    "        from googletrans import Translator\n",
    "        translator = Translator()\n",
    "        # Test the translator\n",
    "        test = translator.translate(\"हेलो\", src='hi', dest='en')\n",
    "        print(\"Google Translate loaded successfully!\")\n",
    "        return (\"google\", translator)\n",
    "    except ImportError:\n",
    "        print(\"Google Translate not available. Installing...\")\n",
    "        try:\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"googletrans==4.0.0-rc1\"])\n",
    "            from googletrans import Translator\n",
    "            translator = Translator()\n",
    "            print(\"Google Translate installed and loaded successfully!\")\n",
    "            return (\"google\", translator)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install Google Translate: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Google Translate error: {e}\")\n",
    "    \n",
    "    # Option 2: Simple dictionary-based translation (basic fallback)\n",
    "    print(\"Using basic dictionary translation (limited accuracy)\")\n",
    "    return (\"basic\", create_basic_translator())\n",
    "\n",
    "def create_basic_translator():\n",
    "    \"\"\"Create a basic Hindi-English dictionary translator\"\"\"\n",
    "    basic_dict = {\n",
    "        'हेलो': 'hello',\n",
    "        'मेरा': 'my',\n",
    "        'नाम': 'name',\n",
    "        'है': 'is',\n",
    "        'मुझे': 'me/I',\n",
    "        'दिन': 'day',\n",
    "        'से': 'from',\n",
    "        'खांसी': 'cough',\n",
    "        'और': 'and',\n",
    "        'ज़ुकाम': 'cold',\n",
    "        'की': 'of',\n",
    "        'शिकायत': 'complaint',\n",
    "        'हो': 'is/being',\n",
    "        'रही': 'happening',\n",
    "        'मेरी': 'my',\n",
    "        'बॉडी': 'body',\n",
    "        'में': 'in',\n",
    "        'भी': 'also',\n",
    "        'पेन': 'pain',\n",
    "        'रहा': 'happening',\n",
    "        'गले': 'throat',\n",
    "        'खराशे': 'irritation',\n",
    "        'सी': 'like',\n",
    "        'लग': 'feeling',\n",
    "        'साथ': 'with',\n",
    "        'ही': 'only/same',\n",
    "        'सर': 'head',\n",
    "        'भारी': 'heavy',\n",
    "        'तो': 'so/then',\n",
    "        'क्या': 'what',\n",
    "        'कोविड': 'COVID',\n",
    "        'या': 'or',\n",
    "        'फिर': 'then',\n",
    "        'कुछ': 'some',\n",
    "        'बीमारी': 'disease',\n",
    "        'होने': 'happening',\n",
    "        'का': 'of'\n",
    "    }\n",
    "    return basic_dict\n",
    "\n",
    "def translate_text(translator_info, hindi_text):\n",
    "    \"\"\"Translate Hindi text to English\"\"\"\n",
    "    if translator_info is None:\n",
    "        return \"Translation not available\"\n",
    "    \n",
    "    translator_type, translator = translator_info\n",
    "    \n",
    "    try:\n",
    "        if translator_type == \"google\":\n",
    "            # Google Translate\n",
    "            result = translator.translate(hindi_text, src='hi', dest='en')\n",
    "            return result.text\n",
    "        \n",
    "        elif translator_type == \"basic\":\n",
    "            # Basic dictionary translation\n",
    "            words = hindi_text.split()\n",
    "            translated_words = []\n",
    "            for word in words:\n",
    "                # Remove punctuation for lookup\n",
    "                clean_word = word.strip('।.,?!').lower()\n",
    "                if clean_word in translator:\n",
    "                    translated_words.append(translator[clean_word])\n",
    "                else:\n",
    "                    translated_words.append(f\"[{word}]\")  # Keep untranslated words in brackets\n",
    "            \n",
    "            return \" \".join(translated_words)\n",
    "        \n",
    "        else:\n",
    "            return \"Unknown translation method\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return f\"Translation failed: {str(e)}\"\n",
    "\n",
    "def load_grounds_truth(file_path):\n",
    "    \"\"\"Load ground truth transcriptions from file\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Ground truth file '{file_path}' not found!\")\n",
    "        return {}\n",
    "    \n",
    "    grounds_truths = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Handle different file formats\n",
    "            if \"|\" in line:\n",
    "                parts = line.split(\"|\", 1)\n",
    "                audio_file, transcription = parts[0].strip(), parts[1].strip()\n",
    "                if not audio_file:\n",
    "                    audio_file = \"audio.flac\"\n",
    "            else:\n",
    "                # No separator found, assume transcription for default audio file\n",
    "                audio_file = \"audio.flac\"\n",
    "                transcription = line\n",
    "            \n",
    "            grounds_truths[audio_file] = transcription\n",
    "    \n",
    "    return grounds_truths\n",
    "\n",
    "def preprocess_audio(audio_path, target_sr=16000):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Convert stereo to mono if needed\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        return waveform, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(model, waveform):\n",
    "    \"\"\"Transcribe audio using the ASR model\"\"\"\n",
    "    # Try different model calling approaches\n",
    "    approaches = [\n",
    "        lambda: model(waveform, \"hi\", \"ctc\"),\n",
    "        lambda: model(waveform.squeeze(), \"hi\", \"ctc\"),\n",
    "        lambda: model(waveform.unsqueeze(0), \"hi\", \"ctc\"),\n",
    "    ]\n",
    "    \n",
    "    for approach in approaches:\n",
    "        try:\n",
    "            result = approach()\n",
    "            return result if isinstance(result, str) else str(result)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"All transcription approaches failed\")\n",
    "\n",
    "def save_translations(translations, output_file=\"translations.txt\"):\n",
    "    \"\"\"Save translations to file\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for audio_file, data in translations.items():\n",
    "            f.write(f\"Audio File: {audio_file}\\n\")\n",
    "            f.write(f\"Hindi: {data['hindi']}\\n\")\n",
    "            f.write(f\"English: {data['english']}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "    print(f\"Translations saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    gt_file = \"grounds_truth.txt\"\n",
    "    target_sr = 16000\n",
    "    output_file = \"translations.txt\"\n",
    "    \n",
    "    # Load ASR model\n",
    "    print(\"Loading ASR model...\")\n",
    "    asr_model = AutoModel.from_pretrained(\"ai4bharat/indic-conformer-600m-multilingual\", trust_remote_code=True)\n",
    "    print(\"ASR model loaded successfully!\")\n",
    "    \n",
    "    # Load translation model\n",
    "    translator = load_translation_model()\n",
    "    \n",
    "    # Dictionary to store all translations\n",
    "    all_translations = {}\n",
    "    \n",
    "    # Load ground truth data (if available)\n",
    "    grounds_truths = load_grounds_truth(gt_file)\n",
    "    \n",
    "    # If we have ground truth, process those files\n",
    "    if grounds_truths:\n",
    "        print(f\"\\nFound {len(grounds_truths)} audio file(s) to process\\n\")\n",
    "        \n",
    "        for audio_file, gt_text in grounds_truths.items():\n",
    "            print(f\"Processing: {audio_file}\")\n",
    "            \n",
    "            # Find audio file\n",
    "            if not os.path.exists(audio_file):\n",
    "                print(f\"Audio file not found: {audio_file}\")\n",
    "                # If we have ground truth text, translate it anyway\n",
    "                print(\"Using ground truth text for translation...\")\n",
    "                hindi_text = gt_text\n",
    "            else:\n",
    "                # Load and transcribe audio\n",
    "                waveform, original_sr = preprocess_audio(audio_file, target_sr)\n",
    "                if waveform is None:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Audio loaded: {waveform.shape[1] / target_sr:.1f}s at {original_sr}Hz\")\n",
    "                \n",
    "                try:\n",
    "                    hindi_text = transcribe_audio(asr_model, waveform)\n",
    "                    hindi_text = hindi_text.strip()\n",
    "                    print(f\"Transcription: {hindi_text}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Transcription failed: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Translate to English\n",
    "            print(\"Translating to English...\")\n",
    "            english_text = translate_text(translator, hindi_text)\n",
    "            \n",
    "            print(f\"Translation: {english_text}\")\n",
    "            all_translations[audio_file] = {\n",
    "                'hindi': hindi_text,\n",
    "                'english': english_text\n",
    "            }\n",
    "            \n",
    "            # Calculate WER/CER if ground truth available\n",
    "            if gt_text and hindi_text != gt_text:\n",
    "                sample_wer = wer(gt_text, hindi_text)\n",
    "                sample_cer = cer(gt_text, hindi_text)\n",
    "                print(f\"WER: {sample_wer:.3f} | CER: {sample_cer:.3f}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    else:\n",
    "        # Process default audio file if no ground truth\n",
    "        audio_file = \"audio.flac\"\n",
    "        if os.path.exists(audio_file):\n",
    "            print(f\"Processing: {audio_file}\")\n",
    "            \n",
    "            waveform, original_sr = preprocess_audio(audio_file, target_sr)\n",
    "            if waveform is not None:\n",
    "                print(f\"Audio loaded: {waveform.shape[1] / target_sr:.1f}s at {original_sr}Hz\")\n",
    "                \n",
    "                try:\n",
    "                    hindi_text = transcribe_audio(asr_model, waveform)\n",
    "                    hindi_text = hindi_text.strip()\n",
    "                    print(f\"Transcription: {hindi_text}\")\n",
    "                    \n",
    "                    # Translate to English\n",
    "                    print(\"Translating to English...\")\n",
    "                    english_text = translate_text(translator, hindi_text)\n",
    "                    \n",
    "                    print(f\"Translation: {english_text}\")\n",
    "                    all_translations[audio_file] = {\n",
    "                        'hindi': hindi_text,\n",
    "                        'english': english_text\n",
    "                    }\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Processing failed: {e}\")\n",
    "            \n",
    "            all_translations[\"sample\"] = {\n",
    "                'hindi': sample_text,\n",
    "                'english': english_text\n",
    "            }\n",
    "    \n",
    "    # Save all translations\n",
    "    if all_translations:\n",
    "        save_translations(all_translations, output_file)\n",
    "        \n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"Files processed: {len(all_translations)}\")\n",
    "        print(f\"Translations saved to: {output_file}\")\n",
    "        \n",
    "        # Display final translations\n",
    "        print(\"\\nFINAL TRANSLATIONS:\")\n",
    "        for audio_file, data in all_translations.items():\n",
    "            print(f\"\\nFile: {audio_file}\")\n",
    "            print(f\"Hindi: {data['hindi']}\")\n",
    "            print(f\"English: {data['english']}\")\n",
    "    else:\n",
    "        print(\"No translations were generated!\")\n",
    "\n",
    "# Simple function to translate just text\n",
    "def quick_translate(hindi_text):\n",
    "    \"\"\"Quick translation function\"\"\"\n",
    "    translator = load_translation_model()\n",
    "    return translate_text(translator, hindi_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbeb782b-1d69-4804-9915-cdfe8ab7d07a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m hindi_transcription \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mहेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में हराशें स ही लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Translate to English\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m english_translation \u001b[38;5;241m=\u001b[39m translate_hindi_to_english(hindi_transcription)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Hindi:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(hindi_transcription)\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mtranslate_hindi_to_english\u001b[1;34m(hindi_text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the translation model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-hi-en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize and translate\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1037\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1037\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1038\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1039\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m             )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def translate_hindi_to_english(hindi_text):\n",
    "    \"\"\"Translate Hindi text to English using Helsinki-NLP model\"\"\"\n",
    "    \n",
    "    # Load the translation model\n",
    "    model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize and translate\n",
    "    inputs = tokenizer(hindi_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the translation\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Input: Predicted transcription from your ASR model\n",
    "hindi_transcription = \"हेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में हराशें स ही लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\"\n",
    "\n",
    "# Translate to English\n",
    "english_translation = translate_hindi_to_english(hindi_transcription)\n",
    "\n",
    "print(\"Original Hindi:\")\n",
    "print(hindi_transcription)\n",
    "print(\"\\nEnglish Translation:\")\n",
    "print(english_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abea37e-0952-463d-8bd3-74c2012d7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Hindi Transcription:\n",
      "हेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में हराशें स ही लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\n",
      "\n",
      "Translating...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Translate to English\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTranslating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m english_translation \u001b[38;5;241m=\u001b[39m translate_hindi_to_english(hindi_transcription)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnglish Translation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(english_translation)\n",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m, in \u001b[0;36mtranslate_hindi_to_english\u001b[1;34m(hindi_text)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load the translation model\u001b[39;00m\n\u001b[0;32m     34\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-hi-en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Tokenize and translate\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1037\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1037\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1038\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1039\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m             )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def extract_prediction_from_notebook(notebook_file):\n",
    "    \"\"\"Extract predicted transcription from Jupyter notebook file\"\"\"\n",
    "    try:\n",
    "        with open(notebook_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        # Look through all cells for prediction output\n",
    "        for cell in notebook['cells']:\n",
    "            if 'outputs' in cell:\n",
    "                for output in cell['outputs']:\n",
    "                    if 'text' in output:\n",
    "                        text_lines = output['text']\n",
    "                        for line in text_lines:\n",
    "                            if line.startswith('Prediction:'):\n",
    "                                # Extract text after \"Prediction: \"\n",
    "                                prediction = line.replace('Prediction:', '').strip()\n",
    "                                return prediction\n",
    "        \n",
    "        print(\"No prediction found in notebook file\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading notebook file: {e}\")\n",
    "        return None\n",
    "\n",
    "def translate_hindi_to_english(hindi_text):\n",
    "    \"\"\"Translate Hindi text to English using Helsinki-NLP model\"\"\"\n",
    "    \n",
    "    # Load the translation model\n",
    "    model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize and translate\n",
    "    inputs = tokenizer(hindi_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the translation\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Automatically extract prediction from notebook file\n",
    "notebook_file = \"med_audi_move.ipynb\"\n",
    "hindi_transcription = extract_prediction_from_notebook(notebook_file)\n",
    "\n",
    "if hindi_transcription:\n",
    "    print(\"Extracted Hindi Transcription:\")\n",
    "    print(hindi_transcription)\n",
    "    \n",
    "    # Translate to English\n",
    "    print(\"\\nTranslating...\")\n",
    "    english_translation = translate_hindi_to_english(hindi_transcription)\n",
    "    \n",
    "    print(\"\\nEnglish Translation:\")\n",
    "    print(english_translation)\n",
    "else:\n",
    "    print(\"Could not extract transcription from notebook file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08980ac2-0506-4734-8233-f8859ca8bf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Hindi Transcription:\n",
      "हेलो मेरा नाम मोहम्मद अतर है मुझे तीन दिन से खांसी और ज़ुकाम की शिकायत हो रही है मेरी बॉडी में भी पेन हो रहा है और गले में हराशें स ही लग रही हैं साथ ही साथ सर भी भारी भारी रह रहा है तो क्या मुझे कोविड है या फिर कुछ और बीमारी होने का\n",
      "\n",
      "Translating...\n",
      "\n",
      "English Translation:\n",
      "Hello, my name is Mohammad Atar, I have been complaining of cough and cold for three days, my body is also getting pen and there is only a green in the neck, as well as the head is also heavy, so do I have covid or some other disease\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import json\n",
    "from googletrans import Translator\n",
    "\n",
    "def extract_prediction_from_notebook(notebook_file):\n",
    "    \"\"\"Extract predicted transcription from Jupyter notebook file\"\"\"\n",
    "    try:\n",
    "        with open(notebook_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        # Look through all cells for prediction output\n",
    "        for cell in notebook['cells']:\n",
    "            if 'outputs' in cell:\n",
    "                for output in cell['outputs']:\n",
    "                    if 'text' in output:\n",
    "                        text_lines = output['text']\n",
    "                        for line in text_lines:\n",
    "                            if line.startswith('Prediction:'):\n",
    "                                # Extract text after \"Prediction: \"\n",
    "                                prediction = line.replace('Prediction:', '').strip()\n",
    "                                return prediction\n",
    "        \n",
    "        print(\"No prediction found in notebook file\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading notebook file: {e}\")\n",
    "        return None\n",
    "\n",
    "def translate_hindi_to_english(hindi_text):\n",
    "    \"\"\"Translate Hindi text to English using Google Translate\"\"\"\n",
    "    try:\n",
    "        translator = Translator()\n",
    "        result = translator.translate(hindi_text, src='hi', dest='en')\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Automatically extract prediction from notebook file\n",
    "notebook_file = \"med_audi_move.ipynb\"\n",
    "hindi_transcription = extract_prediction_from_notebook(notebook_file)\n",
    "\n",
    "if hindi_transcription:\n",
    "    print(\"Extracted Hindi Transcription:\")\n",
    "    print(hindi_transcription)\n",
    "    \n",
    "    # Translate to English\n",
    "    print(\"\\nTranslating...\")\n",
    "    english_translation = translate_hindi_to_english(hindi_transcription)\n",
    "    \n",
    "    print(\"\\nEnglish Translation:\")\n",
    "    print(english_translation)\n",
    "else:\n",
    "    print(\"Could not extract transcription from notebook file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c5936-805e-4294-a8d2-eaa35b473723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
